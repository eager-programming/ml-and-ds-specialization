Вырезано из https://www.coursera.org/learn/supervised-learning/lecture/adARX/gradiientnyi-spusk-dlia-linieinoi-rieghriessii 

Интерактивный текст видеоматериала 

[ЗАСТАВКА] В этом видео мы поговорим о том,  

как применять градиентный спуск для обучения линейной регрессии.  

И давайте начнем с простого примера, со случая, когда признак всего один.  

Эта ситуация называется парной регрессией.  

В этом случае линейная модель выглядит как a от х равняется w 1  

умножить на x плюс w 0.  

То есть мы берем значение нашего единственного признака x,  

умножаем на некоторый вес, некоторый коэффициент w 1,  

и прибавляем свободный коэффициент w 0, который также называется сдвигом.  

Получается, что у нашей модели есть два параметра: w 1 и w 0.  

Их нужно настраивать.  

Функционал ошибки возьмем среднеквадратичный,  

то есть посчитаем квадраты отклонения прогноза модели от истинного ответа  

и усредним по всем объектам.  

Давайте рассмотрим, как будет работать в случае с парной  

регрессией градиентный спуск на примере конкретной выборки.  

Например, это может быть задача предсказания прибыли магазина в следующем  

месяце на основе его прибыли в предыдущем месяце.  

Выборка выглядит вот так.  

По оси x отложено значение единственного признака,  

по оси y отложено значение ответа.  

Если нарисовать функционал качества в осях w 0 и w 1, он будет выглядеть как-то так.  

То есть это некоторая функция параболического вида,  

у нее где-то есть минимум.  

Напомню, градиентный спуск заключается в том, что мы как-то  

инициализируем вектор весов и дальше до сходимости повторяем градиентный шаг.  

То есть вычитаем из текущего приближения вектора весов градиент функционала  

ошибки в этой точке с некоторым коэффициентом в этой точке [INAUDIBLE].  

Сходимость наступает,  

когда вектор весов перестает меняться слишком сильно от одной итерации к другой.  

Чтобы запустить градиентный спуск, нам нужно вычислить градиент,  

то есть вектор частных производных функционала ошибки по всем его параметрам.  

У нас параметра два, w 1 и w 0.  

Мы не будем вдаваться в математические выкладки, можно показать,  

что частные производные записываются вот так.  

Можете проверить дома, что это действительно правда.  

Итак, чтобы посмотреть, как работает градиентный спуск на нашей выборке,  

будем рисовать две картинки.  

На левой картинке мы изображаем пространство параметров.  

По оси x отложен параметр w 0, по оси y параметр — w 1.  

Точка в этом пространстве обозначает конкретную модель.  

Выбирая конкретные места, мы получаем конкретную линейную регрессию.  

На правом графике мы изображаем нашу выборку в осях «признак — ответ»  

и алгоритм, который настроен, который соответствует точке на левом графике.  

Итак, если мы возьмем начальное приближение случайно,  

возьмем в его качестве некоторую точку в окрестности нуля,  

то получим некоторый не очень хороший алгоритм.  

Он совершенно не соответствует тому, какая зависимость есть в нашей выборке.  

Сделаем первый градиентный шаг.  

Он сдвинется вверх и немножко вправо, и мы видим, что после даже первого  

градиентного шага алгоритм гораздо больше соответствует истинной зависимости.  

Направление нашей прямой уже более или менее угадывает общую тенденцию в данных.  

Сделаем еще несколько градиентных шагов.  

Наша точка в пространстве параметра будет двигаться в том же направлении,  

а наша прямая, наш алгоритм,  

будет все ближе и ближе к нашему облаку точек, к нашему облаку объектов.  

После того, как мы сделаем 100 итераций,  

наша точка в пространстве параметров немножко завернет, а сам алгоритм будет  

уже очень неплохо апроксимировать, неплохо приближать данные.  

Видно, что модель после сотой итерации уже довольно неплохая.  

После того, как мы сделаем тысячу итераций, точка в пространстве параметров,  

то есть набор параметров, сдвинется еще сильнее в том же направлении и алгоритм  

станет еще лучше описывать данные.  

После двух тысяч итераций мало что изменится.  

Видно, что уже наступила сходимость, и мы получили довольно неплохой алгоритм,  

который соответствует зависимости между признаком и ответом.  

При этом, если посмотреть на график того,  

как менялось значение функционала ошибки по мере итерации, мы увидим,  

что это изменение было монотонным, начиналось оно в довольно высокой точки,  

затем уменьшалось, и в какой-то момент вышло на асимптоту.  

Очень важно в градиентном спуске понимать, как выбрать размер шага [INAUDIBLE].  

Давайте немножко поговорим об этом.  

Вообще, нет никаких конкретных правил, конкретных рекомендаций,  

каким именно выбрать шаг для данной задачи.  

Выбор шага это искусство, но при этом есть некоторые соображения,  

которые могут помочь.  

Если взять длину шага слишком маленькой, то градиентный спуск будет очень не спеша,  

но верно шагать в сторону минимума, как на этой картинке.  

Видно, что шаги по мере приближения к минимуму становятся все  

более и более маленькими, нужно довольно много итераций,  

чтобы градиентный спуск сошелся с таким размером шага.  

Если же взять размер шага очень большим, то, конечно, градиент будет показывать  

в сторону минимума, но поскольку мы шагаем по нему слишком далеко, есть риск,  

что мы будем перепрыгивать точку минимума, и, более того, есть риск расхождения,  

то есть градиентный спуск будет уходить все дальше и дальше от точки минимума,  

что и происходит на этой картинке.  

Можно заметить, что если мы еще находимся очень далеко от точки минимума,  

то нет ничего плохого в том, чтобы делать длинные шаги,  

чтобы далеко шагать в сторону антиградиента.  

Если же мы уже сделали много итераций градиентного спуска и есть подозрения,  

что находимся близко к точке минимума, то шаги должны быть аккуратными,  

чтобы мы не перескочили минимум, чтобы мы не начали шагать не в ту сторону.  

Таким образом, возникает идея делать шаг переменным.  

Чем больше итераций мы сделали, тем меньше должен быть размер шага.  

Например, можно в этом случае для размера шага [INAUDIBLE] взять формулу k  

поделить на t, где t это номер текущей итерации, а k — некоторая константа.  

видно, что чем больше число итераций, тем меньше будет шаг,  

а константу k нужно как-то подбирать в зависимости от задачи, например,  

пробовать разные значения и посмотреть, когда сходимость есть, а когда нет.  

В случае с многомерной линейной регрессией подход будет тот же самый,  

нужно только по-другому расчитать градиент.  

Функционал в случае с многомерной линейной регрессией, как мы уже выяснили,  

записывается в матричном виде.  

Это будет норма, квадрат нормы отклонения вектора X w,  

то есть вектора прогнозов, от вектора y, то есть вектора истинных ответов.  

И потом еще это делится на l, на размер выборки.  

Нужно минимизировать этот функционал ошибки.  

Можно показать,  

что градиент этого функционала в точке w вычисляется по вот такой формуле.  

Нужно взять матрицу X, то есть матрицу «объекты — признаки», транспонировать ее,  

умножить на вектор отклонений X w — y,  

то есть на вектор ошибок алгоритма на каждом объекте обучения,  

и потом умножить все это на скаляр 2 поделить на l.  

Именно вдоль этого вектора нужно шагать на каждом шаге градиентного спуска.  

Итак, мы с вами обсудили, как будет выглядеть градиентный спуск для парной,  

то есть одномерной, и многомерной линейной регрессии, обсудили важность выбора шага.  

А в следующем видео мы поговорим о модификации градиентного спуска,  

стохастическом градиентном спуске,  

который хорошо подходит для настройки линейной регрессии.  
