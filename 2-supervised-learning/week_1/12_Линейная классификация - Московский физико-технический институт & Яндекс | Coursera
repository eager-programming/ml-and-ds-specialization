Вырезано из https://www.coursera.org/learn/supervised-learning/lecture/jqLcO/linieinaia-klassifikatsiia 

Интерактивный текст видеоматериала 

[ЗАСТАВКА] В  

этом видео мы поговорим про линейную классификацию: то есть как применять  

линейные модели к задачам классификации.  

И будем говорить о самом простом виде классификации — бинарной классификации,  

где ответы принимают значения из множества −1 и +1,  

то есть всего 2 возможных значения.  

Как вы помните, чтобы работать с той или иной моделью,  

нужно уметь отвечать на 3 вопроса: первый — это как мы измеряем качество,  

как устроен функционал ошибки; второй — как устроено семейство алгоритмов,  

то есть то множество алгоритмов, из которого мы выбираем наилучший  

с точки зрения функционала; и третий — это как мы обучаем алгоритм,  

то есть выбираем лучший из семейства с точки зрения функционала ошибки.  

В этом видео мы поговорим о семействе алгоритмов, а в следующем — о том,  

как измерять их ошибки и как обучать эти алгоритмы.  

Мы уже говорили про линейную регрессию.  

Линейные классификаторы устроены очень похоже.  

В линейной регрессии мы складывали все признаки с весами,  

при этом вес при j-том признаке обозначали как wj-тое, и после этого суммирования  

проявляли еще свободный коэффициент w0, который также называется сдвигом.  

В случае с регрессией нас эта формула полностью устраивала,  

поскольку она принимала вещественные значения,  

теперь же алгоритм должен возвращать бинарные значения: −1 или +1.  

Чтобы добиться этого, можно просто взять знак от этого выражения,  

именно это и будет видом линейного классификатора.  

Заметим, что формула не очень однородная, в ней есть свободный коэффициент,  

чтобы убрать его, давайте просто добавим еще один признак выборки, константный  

признак, который на каждом объекте принимает значение 1, единичный признак.  

В этом случае свободный коэффициент уже не нужен, его роль будет выполнять вес при  

этом константном признаке, и формула приобретает такое значение.  

Заметим, что по сути такая взвешенная сумма признаков —  

это скалярное произведение вектора весов на вектор признаков, и значит итоговый  

вид линейного классификатора — это знак скалярного произведения w на x,  

этой формулой мы и будем пользоваться дальше.  

Давайте разберемся, какой геометрический смысл у линейного классификатора.  

В случае с линейной регрессией мы обсуждали, что это по сути приближение  

зависимости ответа от признаков с помощью прямой или гиперплоскости.  

Что же это будет в случае с классификацией?  

Для этого давайте запишем то, что стоит под функцией знака — скалярное  

произведение w на x, и приравняем к 0, получим такое уравнение.  

Давайте нарисуем вектор весов w и будем искать все такие точки x,  

которые удовлетворяют этому уравнению, то есть все такие точки,  

для которых скалярное произведение этой точки, этого вектора на w = 0.  

Равенство нулю скалярного произведения означает,  

что угол между этими векторами = 90 градусов, то есть что они перпендикулярны.  

Получается, что точки, удовлетворяющие этому уравнению — это все векторы,  

ортогональные вектору весов.  

Из аналитической геометрии известно,  

что это множество представляет собой плоскость.  

Получается, что уравнение задает плоскость, более того,  

с одной стороны от этой плоскости значение скалярного произведения будет больше 0,  

а с другой стороны от плоскости — меньше 0.  

Получается, что линейный классификатор проводит гиперплоскость в пространстве  

признаков, и все объекты, которые с одной стороны, относят к классу +1,  

а те, которые с другой стороны — к классу −1.  

В случае с двумя признаками это выглядит как-то так: у нас есть выборка,  

мы проводим разделяющую прямую, и все объекты, которые с одной стороны,  

относим к классу −1, все, которые с другой стороны, относим к классу +1.  

Заметим, что линейные классификаторы вычисляют  

значение скалярного произведения, которое имеет вещественное значение,  

а затем берет только знак, отбрасывая часть информации.  

При этом, наверное, само значение скалярного произведения тоже имеет смысл.  

Действительно, оказывается, что если мы возьмем модуль этого скалярного  

произведения и отнормируем его, то есть поделим на норму вектора весов,  

то это выражение будет равно расстоянию от точки x до гиперплоскости,  

которая задается вектором нормали, вектором весов w.  

Получается, что линейный классификатор сначала измеряет расстояние от точки  

до гиперплоскости со знаком и дальше смотрит лишь на знак,  

то есть на то, с какой стороны от гиперплоскости лежит эта точка.  

Так мы приходим к очень важному понятию в линейной классификации  

— к понятию отступа.  

Отступом называется выражение вида: скалярное произведение вектора  

весов на объект, умноженное на истинный ответ на этом объекте,  

который, напомню, равен +1 и −1.  

Какой смысл у этого выражения?  

Давайте обратим внимание: если скалярное произведение имеет положительный  

знак и истинный ответ равен +1 — это верная классификация и  

произведение скалярного произведения на истинный ответ будет больше 0.  

Если скалярное произведение меньше 0, и истинный ответ равен −1, то это тоже  

будет правильная классификация, и их произведение снова будет больше 0.  

Если же знак ответа и знак скалярного произведения противоположные,  

то классификация будет ошибочной и знак отступа будет меньше 0.  

Получается, что отступ — это некоторая величина,  

которая характеризует корректность ответа.  

Если отступ больше 0, то алгоритм дает корректный ответ,  

если отступ меньше 0 — алгоритм дает некорректный ответ.  

При этом само абсолютное значение отступа свидетельствует о  

расстоянии от точки до разделяющей гиперплоскости.  

Принято считать, что если точка находится рядом с разделяющей гиперплоскостью,  

то классификация неуверенная, наш алгоритм сомневается, к какому классу относить ее,  

если же точка находится далеко от разделяющей гиперплоскости,  

то классификация уверенная, при этом, если алгоритм прав, то он просто уверен в этом,  

все хорошо, если же алгоритм ошибается, и при этом отступ по модулю очень большой,  

это означает, что алгоритм очень сильно ошибается в классификации этого объекта,  

возможно, этот объект является выбросом и никак не вписывается в нашу модель,  

или же алгоритм не подходит для решения этой задачи.  

Итак, мы с вами обсудили, что линейный классификатор по сути строит  

гиперплоскость в пространстве признаков и с ее помощью разделяет 2 класса.  

Также мы ввели понятие отступа, знак которого  

позволяет понять — корректный ответ дает классификатор или нет на этом объекте,  

а абсолютное значение отступа говорит об уверенности классификатора в этом объекте.  

В следующем видео мы поговорим о том, как измерять качества,  

как измерять ошибку линейного классификатора и как его настраивать.  
