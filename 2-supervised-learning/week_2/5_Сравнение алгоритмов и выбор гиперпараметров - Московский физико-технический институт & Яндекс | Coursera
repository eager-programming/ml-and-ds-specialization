Вырезано из https://www.coursera.org/learn/supervised-learning/lecture/aF79U/sravnieniie-alghoritmov-i-vybor-ghipierparamietrov 

Интерактивный текст видеоматериала 

[ЗАСТАВКА] В этом видео мы поговорим  

о том, как выбирать гиперпараметры и сравнивать разные алгоритмы с помощью  

уже изученных нами схем — отложенной выборки или кросс-валидации.  

Давайте начнем с того, что разберемся, что такое гиперпараметры.  

Так называются те параметры алгоритмов,  

которые нельзя настраивать по обучающей выборке.  

Простым примером гиперпараметра является параметр регуляризации,  

поскольку регуляризатор штрафует модель и не дает ей слишком сильно подогнаться под  

обучающую выборку, понятно, что с точки зрения ошибки на обучение оптимально  

выставить параметр регуляризации в 0, то есть выключить регуляризатор.  

Другой пример гиперпараметра — это, например, степень полинома,  

с помощью которого мы описываем данные.  

В нашем примере про переобучение с точки зрения обучающей выборки оптимально было  

брать полином в степени 9, поскольку он проходил через через все точки обучения,  

давал нулевую ошибку на обучение.  

Но при этом понятно, что обобщающая способность у него была никакая.  

Более общая задача — это сравнение разных алгоритмов, например,  

сравнение качества алгоритмов, настроенных с разными значениями гиперпараметров.  

Или, предположим, вы можете настраивать алгоритмы на среднеквадратичную ошибку и  

на среднеабсолютную ошибку и пытаться понять, что из этого лучше.  

Или выбирать типы регуляризации: L2 или L1 — и тоже как-то сравнивать алгоритмы,  

обученные с разными видами регуляризации.  

Или сравнивать разные классы моделей, например,  

линейные модели и решающие деревья, которые будем изучать в следующем модуле.  

Всё это называется сравнением алгоритмов.  

Понятно, что для этого нужно использовать либо обучающую...  

либо отложенную выборку, либо кросс-валидацию,  

но при этом нужно соблюдать осторожность.  

Вот почему: давайте рассмотрим пример, в котором мы сравниваем  

тысячу разных типов алгоритмов с помощью отложенной выборки.  

Каждый их них мы обучаем на обучающей выборке, измеряем качество на отложенной  

выборке и дальше выбираем из них лучшие по качеству на отложенной выборке.  

При этом заметьте, что отложенная выборка, по сути, превратилась в обучающую.  

У нас было большое семейство алгоритмов,  

и мы из них выбрали лучшие отложенные выборки.  

По сути, мы подогнались под нее,  

и у нас снова появился риск переобучения под отложенную выборку.  

Чтобы бороться с этим,  

нужно немножко усовершенствовать нашу схему оценивания качества.  

Разобьем все данные на три части: на обучение, валидацию и контроль.  

Каждый из нашей тысячи алгоритмов будем обучать на обучающей выборке и  

измерять качество на валидационной выборке.  

Получим тысячу показателей качества и по ним выберем лучший из этой тысячи  

алгоритмов — тот, который допускает наименьшую ошибку.  

После того, как лучший алгоритм выбран, мы измерим его качество на контрольной  

выборке и проверим его на адекватность, то есть что оно устраивает нас.  

По сути, именно контрольная выборка будет играть роль новых данных.  

Например, можно проверять, что доля ошибки не слишком большая и отличается от одной  

второй, то есть от случайного угадывания, от подбрасывания монетки.  

Или что среднеквадратичная ошибка лучше, чем у константного алгоритма.  

Если же в вашей схеме предпочтительней использовать кросс-валидацию,  

нужно разбить выборку на две части.  

Одна — это контрольная, на которой будет измеряться качество итогового алгоритма,  

а другая — та, на которой будет делаться кросс-валидация.  

То есть по стандартной схеме разбивается на K блоков,  

каждый блок по очереди выступает в качестве контрольного,  

на нём измеряется качество, а алгоритмы обучены по всем остальным блокам.  

После того как по кросс-валидации выбран лучший из множества алгоритм,  

его адекватность измеряется на контрольной выборке.  

Итак, мы выяснили, что для выбора гиперпараметров или сравнения алгоритмов  

нужно использовать стандартные схемы: отложенную выборку или кросс-валидацию.  

Но при этом, если вы сравниваете очень много разных моделей, есть риск  

переобучения, и чтобы его избежать, нужно выделять контрольную выборку,  

на которой вы проверяете итоговый алгоритм на адекватность.  

На этом урок про сравнение моделей заканчивается, и вы узнали много нового.  

Например, как использовать регуляризацию, чтобы бороться с переобучением линейных  

моделей, или как строить схему проверки качества, которая позволит понять,  

насколько хорошо алгоритм будет работать на новых данных.  
