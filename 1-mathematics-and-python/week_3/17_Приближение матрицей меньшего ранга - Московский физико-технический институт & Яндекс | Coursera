Вырезано из https://www.coursera.org/learn/mathematics-and-python/lecture/Tgj0D/priblizhieniie-matritsiei-mien-shiegho-rangha 

Video Player is loading. 

Play Video 

Loaded: 0% 

0:03 

Progress: 0% 

Current Time 0:03 

/ 

Duration 5:20 

Субтитры 

Отключить субтитры 

Русский 

Качество видео 

Среднее 

Скорость воспроизведения 

1.00x 

Автозапуск 

This is a modal window. 

Beginning of dialog window. Escape will cancel and close the window. 

TextColorTransparencyBackgroundColorTransparencyWindowColorTransparency 

Font SizeText Edge StyleFont Family 

End of dialog window. 

Не удалось отправить. Повторите попытку.Пропустить 

В этом видео мы увидим  

матричные разложения немного с другого ракурса.  

А начнем мы говорить про приближение матрицы,  

некоторой другой матрицы меньшего ранга.  

Для начала давайте сообразим все же, что такое ранг.  

Ну с этим понятием мы уже знакомились,  

и интуитивно понятно, что матрица задает некоторые отображения,  

и ранг каким-то образом описывает «сложность» этого отображения.  

Если более формально, то ранг матрицы — это максимальное  

количество линейно независимых столбцов или строк матрицы.  

Можно ввести ранг и по-другому.  

Можно сказать,  

что ранг — это максимальный размер подматрицы с ненулевым определителем.  

Также полезно заметить, что если размер матрицы X — m на n,  

то ранг матрицы будет точно не больше, чем минимум из этих двух чисел,  

что в целом понятно из определения.  

В случае, если мы рассматриваем произведение двух матриц,  

ну например, матрица A имеет размер m на k, матрица B имеет размер k на n,  

то их произведение конечно имеет размер m на n.  

Однако, если число k очень маленькое, ну то есть меньше m и n,  

то ранг этого произведения тоже не может быть больше, чем k.  

Итак, зачем  

нам может быть нужно приближать матрицу матрицей меньшего ранга?  

А у нас может быть предположение, что матрица,  

которую мы видим в действительности, не совсем правильная.  

Ну например,  

это могут быть какие-нибудь рейтинги фильмов или какие-нибудь признаки.  

И мы можем думать, что там есть какой-то шум.  

И нам хочется восстановить более простую матрицу,  

которая будет достаточно близка к той, которую мы пронаблюдали.  

Один из способов это делать заключается в том,  

что мы не просто пытаемся подобрать матрицу меньшего ранга k,  

а мы представляем ее как произведение двух матриц U и V транспонированное  

таких размеров, чтобы ранг произведения был не больше, чем k.  

Но что значит «приблизить»?  

Что значит, что X будет примерно равно U умножить на V транспонированное?  

Ну просто нам нужно ввести некоторую норму на матрицах, сказать,  

что норма разности X − U * V транспонированное должна быть  

минимальной и подбирать матрицы U и V таким образом.  

Как ввести норму матрицы?  

На самом деле есть много способов.  

Один из них — это норма Фробениуса.  

Выглядит достаточно интуитивно.  

Нужно просто взять и просуммировать квадраты  

всех элементов матрицы и извлечь из этого всего корень.  

Таким образом, с нормой Фробениуса задача принимает следующий вид: U и V должны  

просто минимизировать сумму по всем i, j квадратов отклонения значений  

в матрице xᵢⱼ от значений в произведении матриц.  

Пример первый: преобразование признаков.  

Если исходная матрица X — это матрица с признаками некоторых объектов,  

то новая матрица U содержит столько же строчек, сколько и в ней,  

и тоже может интерпретироваться как матрица признаков объектов.  

При этом, если число k у нас намного меньше,  

чем n (исходное количество признаков),  

то мы еще и выполняем задачу понижения размерности пространства признаков.  

Так как по матрице U можно довольно неплохо восстановить матрицу X с помощью  

матрицы V, то мы выполняем задачу понижения размерности таким образом,  

что теряем как можно меньше полезной информации.  

Ну то есть практически все нужное нам сохраняем.  

Другой пример — задача рекомендаций.  

Пусть X — это матрица с оценками xᵢⱼ, поставленными пользователем i фильму j.  

Какие-то значения матрицы нам известны, а какие-то остаются для нас тайной.  

Возьмем простую модель.  

Будем приближать xᵢⱼ произведением uᵢ на vⱼ,  

где uᵢ отражает интересы пользователя, а vⱼ отражает некоторые признаки фильма.  

Возникает следующая идея: давайте настроим u и v таким образом,  

чтобы хорошо прогнозировать известные значения xᵢⱼ, а неизвестные значения  

спрогнозируем как произведение уже настроенных векторов признаков.  

Тогда задача немного модифицируется.  

Теперь мы берем сумму не по всем элементам ij, а только по тем, которые нам известны.  

Подведем итог.  

Ранг матрицы — это некоторый способ описать «сложность» матрицы.  

Матричными разложениями называют не только представление матрицы произведением  

других, но и приближение исходной матрицы некоторым произведениям,  

как правило, более низкого ранга.  

В следующем видео мы узнаем, как с этими матричными разложениями  

связано сингулярное разложение из прошлого видео.  
