Вырезано из https://www.coursera.org/learn/mathematics-and-python/lecture/uyr7q/primienieniie-ghradiienta 

This is a modal window. 

Beginning of dialog window. Escape will cancel and close the window. 

TextColorTransparencyBackgroundColorTransparencyWindowColorTransparency 

Font SizeText Edge StyleFont Family 

End of dialog window. 

[БЕЗ_ЗВУКА] Итак,  

мы с вами уже знаем, что такое частные производные и градиент.  

Пора узнать, зачем же они нужны.  

Представим, что мы решаем некоторую задачу оптимизации.  

Ну, например, у нас есть функция от многих переменных, x1,  

..., xn, и мы хотим её минимизировать.  

Ну откуда может взяться такая задача?  

Например, мы можем выбирать оптимальные параметры рекламной кампании или подбирать  

веса классификатора текстов, минимизируя ошибку классификации, или настраивать  

параметры сложного технологического процесса, минимизируя вероятность неудачи.  

Но как же нам выбрать параметры таким образом, чтобы функция была минимальной?  

Ну давайте посмотрим на ситуацию в простом двумерном случае,  

то есть в случае функции от двух переменных x и y.  

Если зафиксировать значение x и посмотреть на всё в координатах z и y,  

и если зафиксировать значение y и посмотреть на всё в координатах z и x,  

из графиков станет понятно,  

что значения частных производных в точке экстремума должны быть нулевыми.  

Так как все частные производные должны равняться 0, можно просто сказать,  

что градиент должен быть нулевым вектором.  

Но бывают ситуации, когда функция не  

выписывается аналитически или когда полученные уравнения слишком сложны.  

Что же делать в этом случае?  

В этом случае вам поможет численная оптимизация.  

Так как градиент — это направление наискорейшего роста,  

а антиградиент — это направление наискорейшего убывания,  

то давайте просто начнём в произвольной точке (x1, ..., xN),  

и на каждом шаге будем двигаться в сторону антиградиента.  

Таким образом, значение x  

n+1 (нашего аргумента функции, векторного аргумента)  

будет получаться из прошлого значения xn просто вычитанием антиградиента,  

умноженного на некоторую величину, называемую шагом градиентного спуска.  

Таким образом, мы будем постепенно приближаться к минимуму и в идеальной  

картине мира, конечно, его достигнем.  

У этого всего есть простая аналогия.  

Представьте, что вы приехали в незнакомое место и поселились где-то в низине.  

Гуляя по недалёким окрестностям, вы заблудились и теперь хотите попасть домой.  

Понятно, что если вы не ориентируетесь в местности,  

самым логичным решением будет идти по самому крутому уклону вниз.  

Таким образом вы, конечно, настигните место, где поселились.  

Конечно, есть вопрос, насколько это будет безопасно,  

но, кстати, если говорить о безопасности, то задача экстремума с  

дополнительными условиями — это задача условного экстремума,  

и об этом будет рассказано в дополнительной лекции.  

Итак.  

Подведём итог.  

Мы познакомились с вами с необходимым условием минимума  

функции многих переменных, а также узнали, как работает градиентный спуск,  

Отличный алгоритм численной оптимизации,  

который будет использоваться в нашей специализации повсеместно.  
