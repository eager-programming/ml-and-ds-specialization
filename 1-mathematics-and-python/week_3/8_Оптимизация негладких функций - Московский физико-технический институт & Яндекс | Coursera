Вырезано из https://www.coursera.org/learn/mathematics-and-python/lecture/0ZFvR/optimizatsiia-nieghladkikh-funktsii 

This is a modal window. 

Beginning of dialog window. Escape will cancel and close the window. 

TextColorTransparencyBackgroundColorTransparencyWindowColorTransparency 

Font SizeText Edge StyleFont Family 

End of dialog window. 

[БЕЗ_ЗВУКА] Настало время узнать,  

что не все функции одинаково хороши.  

Представим, что мы обучаем какой-нибудь алгоритм машинного обучения,  

благо мы их расскажем в оставшейся части специализации целую уйму,  

и у него есть какие-то параметры α1, α2, ..., αN.  

Эти параметры могут быть самыми разными: действительные числа, целые числа,  

просто нолики и единички — ну в общем, какие-то числа.  

И хотим мы их подобрать таким образом,  

чтобы качество работы нашего алгоритма было максимальным.  

Казалось бы, причем здесь математика?  

Но оказывается, значений параметров может быть очень много,  

комбинаций очень много, и поэтому просто в лоб перебрать достаточно затруднительно.  

И это повод для оптимизационного процесса.  

Давайте просто рассмотрим качество работы алгоритма как функцию от его параметров  

и будем подбирать параметры так, чтобы максимизировать качество.  

Ну или если мы измеряем ошибку алгоритма, то тогда подбираем параметры так,  

чтобы минимизировать ошибку.  

Неважно, в любом случае мы решаем задачу поиска экстремума.  

Мы с вами уже знаем один из методов численной оптимизации  

— это градиентный спуск.  

Поможет ли нам градиентный спуск в произвольном случае?  

Ну на самом деле наша функция от параметров запросто  

может не иметь градиента, может быть негладкой.  

Кроме того, может быть достаточно затруднительно этот градиент вычислять,  

даже если он есть.  

Помимо этого, существует также проблема локальных минимумов.  

Локальные минимумы — конечно, минимумы, но попасть-то хочется в глобальный,  

потому что мы хотим получить наименьшее значение функции, и, конечно же,  

градиентный спуск ничего особо не может сделать с этими локальными минимумами.  

Ну вот попал он на дно локального минимума, ну теперь градиент равен нулю,  

ну и как оттуда выбраться?  

Поэтому возникает идея придумать какие-нибудь новые методы,  

например методы случайного поиска.  

Общая идея этих методов заключается в том, что на каждом следующем  

шаге мы переходим в некоторую случайную точку, но с такой вероятностью,  

чтобы в среднем мы двигались туда, куда нужно, то есть к минимуму.  

Ну то есть вероятность перехода в новую точку зависит от того,  

какое значение функции там есть.  

Проиллюстрирую это одним наглядным примером.  

На шаге 0 мы зафиксируем  

какой-то числовой параметр d и выберем вектор u,  

который будет задавать некоторое направление случайным образом  

из равномерного распределения на единичной сфере.  

Подробнее о распределениях будет рассказано в уроках про теорию  

вероятности.  

На следующем шаге мы просто сместимся в  

направлении вектора u на величину, которая показана на слайде.  

Обратите внимание: мы нигде не вычисляем  

градиент, и смещаемся мы в каких-то случайных направлениях,  

но за счет того, что величина этого смещения зависит от значения  

функции в точке (x + d * u), мы добиваемся того,  

что в среднем мы двигаемся по антиградиенту, ну если он есть.  

Если его нет, то в среднем мы двигаемся по антиградиенту сглаженной функции.  

Остается понять, что же меняет параметр d.  

Параметр d влияет на то, как сильно мы сглаживаем функцию.  

Как вы видите на слайде, чем больше параметр d, тем больше сглаживание.  

Ну действительно, если мы посмотрим на величину шага, то мы сразу же поймем,  

что это некоторая численная оценка для производной функции f в направлении u, и,  

в зависимости от d, мы просто  

отступаем на разную величину от исходной точки.  

Таким образом, выбирая большие d,  

мы отступаем больше и функция получается более сглаженной.  

Подведем итог.  

Мы обсудили тот факт, что не все функции одинаково хороши,  

не на всех из них будет хорошо работать градиентный спуск.  

Также мы обсудили проблему локальных минимумов, с которой градиентный спуск не  

очень хорошо справляется, и теперь мы знаем еще один метод оптимизации,  

метод случайного поиска, который позволяет нам как-то эту проблему решать.  
