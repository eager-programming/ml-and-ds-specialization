Вырезано из https://www.coursera.org/learn/mathematics-and-python/lecture/zuTEu/sviaz-singhuliarnogho-razlozhieniia-i-priblizhieniia-matritsiei-mien-shiegho 

Video Player is loading. 

Play Video 

Loaded: 0% 

0:02 

Progress: 0% 

Current Time 0:02 

/ 

Duration 6:09 

Субтитры 

Отключить субтитры 

Русский 

Качество видео 

Среднее 

Скорость воспроизведения 

1.00x 

Автозапуск 

This is a modal window. 

Beginning of dialog window. Escape will cancel and close the window. 

TextColorTransparencyBackgroundColorTransparencyWindowColorTransparency 

Font SizeText Edge StyleFont Family 

End of dialog window. 

Не удалось отправить. Повторите попытку.Пропустить 

[БЕЗ_ЗВУКА] Теперь давайте узнаем,  

как сингулярное разложение матриц связано с низкоранговыми приближениями.  

Итак, если мы приближаем исходную матрицу (матрицу меньшего ранга),  

то мы просто пытаемся найти некоторую матрицу X с крышкой ранга не больше,  

чем k, которая будет не сильно отличаться от исходной по какой-то норме.  

В случае матричного разложения мы рассматриваем произведение матриц  

U х V транспонированное как матрицу, которую мы приближаем, исходную.  

Давайте вспомним, что такое SVD,  

или сингулярное разложение матриц.  

Сингулярное разложение матриц — это представление исходной матрицы  

произведением трех других: ортогональной, диагональной и снова ортогональной.  

Оказывается, что если мы рассматриваем матрицу Xk,  

которая представляет из себе произведение трех матриц,  

получаемых из SVD, она будет обладать некоторыми особыми свойствами.  

Что же это за три матрицы?  

Давайте из первой матрицы, из SVD, возьмем k столбцов первых,  

из второй матрицы возьмем квадрат размера k × k,  

он должен соответствовать самым большим диагональным числам,  

и из третьей матрицы возьмем k строк.  

И вот эти вот матрицы и перемножим, получив матрицу Xk.  

Так что же за удивительное свойство, которым она будет обладать?  

Оказывается, матрица Xk будет наилучшим приближением исходной матрицы  

X по норме Фробениуса.  

Это совершенно нетривиальный на первый взгляд факт,  

но его можно с успехом использовать в разных прикладных ситуациях.  

Да, напомню, что норма Фробениуса — это просто корень из суммы  

квадратов координат матрицы.  

Соответственно, норма Фробениуса разности матриц — это просто корень из суммы  

квадратов разностей.  

Таким образом, SVD позволяет нам  

сделать наилучшее приближение исходной матрицы по норме Фробениуса.  

В рекомендательных системах, используя матричные разложения,  

мы тоже ищем наилучшее приближение.  

Наверняка это неспроста.  

Есть такой вариант: давайте возьмем SVD, как-то распределим  

диагональную матрицу между двумя другими и будем получившийся результат  

использовать как матричное разложение, например в рекомендательных системах.  

Первый недостаток такой мысли в том, что SVD не то чтобы очень просто сделать.  

Второй недостаток заключается в том, что SVD мы делаем для всей матрицы,  

а некоторые значения нам неизвестны, значит вместо них нам все равно придется  

подставить какие-то искусственные значения — нули или среднее значение.  

Однако так сложилось, что очень долго  

в рекомендациях любое матричное разложение (на две матрицы, не на три) называли SVD.  

Ну, видимо, из-за вот этого свойства SVD.  

Однако мы призываем все же говорить «матричное разложение  

матрицы предпочтения», а не SVD.  

Итак, как же нам сделать рекомендации?  

Настал момент, когда мы можем с этим разобраться.  

Первый вариант будет не очень хорош по качеству, но он будет значительно проще.  

Мы можем сделать SVD разложение исходной матрицы предпочтений,  

воспользовавшись какой-нибудь готовой реализацией SVD,  

и матрицу UkDk использовать как матрицу профилей пользователей,  

а матрицу Vk — как матрицу профилей фильмов.  

А произведение профилей будет нашей оценкой для того,  

насколько понравится фильм пользователю.  

Ну на самом деле матрицу D мы можем распределять и как-то по-другому.  

Другой вариант более правильный.  

Мы можем никак не использовать SVD как метод,  

а просто подбирать U и V, минимизируя функционал.  

Обратите внимание: даже в SVD мы  

можем по-разному распределять диагональную матрицу по двум другим,  

а если мы просто рассмотрим некоторое матричное разложение,  

ну то есть пусть некоторая матрица X = матрице A * матрицу B,  

то домножение матрицы  

A на некоторую матрицу R (справа) и домножение матрицы B на некоторую матрицу  

R в минус первой (слева) не приведет к изменению произведения.  

Таким образом,  

мы просто найдем еще один способ разложить матрицу X в произведение двух матриц.  

И если у этих матриц также будут те же хорошие свойства, что и у матриц A и B,  

то это будет просто еще одним решением задачи разложения исходной матрицы.  

Таким образом, задача в принципе не имеет однозначного решения в общем случае,  

и с этим нужно как-то бороться.  

О том, как с этим быть, мы поговорим еще в курсе про прикладные задачи.  

Подведем итог.  

Сингулярное разложение матрицы позволяет построить наилучшее  

приближение некоторой матрицы матрицей ранга k по норме Фробениуса.  

Это свойство сингулярного разложения привело к распространению термина  

«SVD» в рекомендательных системах, хотя там, конечно, речь о двух матрицах,  

а не о трех.  

Это же свойство может быть использовано и просто для понижения размерности  

пространства признаков.  

На этом мы заканчиваем с вами знакомство с матричными разложениями.  

Мы с вами узнали, что матричные разложения можно интерпретировать по-разному,  

что они бывают тоже разные.  

И даже достаточно плотно познакомились с сингулярным разложением  

матриц и с его применениями в рекомендательных системах.  
